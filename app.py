# --------------------------------------------------------------------------
# DerivaAI - Professor Particular de C√°lculo
#
# Fase 1 - MVP Robusto com Banco de Dados
#
# Este script representa a aplica√ß√£o funcional conectada a um banco de 
# dados PostgreSQL para persist√™ncia de dados.
# --------------------------------------------------------------------------

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain.prompts import ChatPromptTemplate
from sqlalchemy.sql import text # Para executar SQL de forma segura

# --- 1. CONFIGURA√á√ÉO INICIAL E CHAVES DE API ---

# Carrega a chave da API da OpenAI a partir dos "Secrets" do Streamlit
# Este √© o m√©todo seguro para usar credenciais em produ√ß√£o.
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except KeyError:
    st.error("A chave de API da OpenAI n√£o foi encontrada. Por favor, configure-a nos Secrets do Streamlit.")
    st.stop()

# TODO: Substituir user_id hardcoded por um sistema de autentica√ß√£o din√¢mico no Passo 3.
user_id = 1 

# --- 2. CONFIGURA√á√ÉO DA P√ÅGINA E ESTILOS ---

st.set_page_config(
    page_title="DerivaAI",
    page_icon="üß†",
    layout="wide"
)

# Fun√ß√£o para carregar CSS a partir de um arquivo externo
def load_css(file_path="style.css"):
    try:
        with open(file_path, encoding='utf-8') as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    except FileNotFoundError:
        st.warning(f"Arquivo de estilo '{file_path}' n√£o encontrado.")

# --- 3. L√ìGICA DO CHATBOT (LANGCHAIN E OPENAI) ---

# Carrega o prompt principal do sistema a partir de um arquivo de texto
def carregar_prompt_sistema(file_path="prompt_revisado.txt"):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # Substituir chaves para evitar conflito com o f-string do Python
            return f.read().replace("{", "{{").replace("}", "}}")
    except FileNotFoundError:
        st.error(f"Arquivo de prompt '{file_path}' n√£o encontrado. Verifique se ele est√° no reposit√≥rio.")
        st.stop()

# Inicializa o modelo de linguagem (LLM) da OpenAI
llm = ChatOpenAI(
    temperature=0,
    model="gpt-4o-mini",
    api_key=OPENAI_API_KEY
)

# Cria o template do prompt que ser√° usado na cadeia
template = ChatPromptTemplate.from_messages([
    ('placeholder', '{memoria}'),
    ('system', carregar_prompt_sistema()),
    ('human', '{pergunta}')
])

# Cria a cadeia de conversa√ß√£o que une o template e o LLM
chain_memoria = template | llm

# --- 4. FUN√á√ïES DE INTERA√á√ÉO COM O BANCO DE DADOS (POSTGRESQL) ---

def salvar_mensagem(user_id, tipo, conteudo):
    """
    Salva uma nova mensagem no banco de dados PostgreSQL.
    'tipo' pode ser 'user' ou 'ai'.
    """
    try:
        conn = st.connection("postgres", type="sql")
        with conn.session as s:
            s.execute(
                text('INSERT INTO conversas (user_id, sender, message_content) VALUES (:user_id, :sender, :content);'),
                params=dict(user_id=user_id, sender=tipo, content=conteudo)
            )
            s.commit()
    except Exception as e:
        st.error(f"Erro ao salvar mensagem: {e}")

def carregar_mensagens(user_id):
    """
    Carrega o hist√≥rico de mensagens de um usu√°rio espec√≠fico do banco de dados.
    Retorna uma lista de tuplas (sender, message_content).
    """
    try:
        conn = st.connection("postgres", type="sql")
        df = conn.query(
            'SELECT sender, message_content FROM conversas WHERE user_id = :user_id ORDER BY timestamp;',
            params={"user_id": user_id},
            ttl=0  # Desativa o cache para sempre ter o hist√≥rico mais recente
        )
        return [(row.sender, row.message_content) for index, row in df.iterrows()]
    except Exception as e:
        st.error(f"Erro ao carregar mensagens: {e}")
        return []

# --- 5. L√ìGICA DA P√ÅGINA DE CHAT (INTERFACE DO USU√ÅRIO) ---

def pagina_chat():
    st.header("Bem-vindo ao DerivaAI! üß†")

    # Inicializa a mem√≥ria da conversa para a sess√£o atual
    memoria = ConversationSummaryBufferMemory(
        llm=llm,
        max_token_limit=1000,
        return_messages=True
    )

    # Carrega o hist√≥rico de mensagens do banco de dados e exibe na tela
    mensagens_salvas = carregar_mensagens(user_id)
    for tipo, conteudo in mensagens_salvas:
        # Adiciona a mensagem √† mem√≥ria do LangChain para manter o contexto
        if tipo == "user":
            memoria.chat_memory.add_user_message(conteudo)
        elif tipo == "ai":
            memoria.chat_memory.add_ai_message(conteudo)
        
        # Exibe a mensagem na interface do usu√°rio
        with st.chat_message(tipo):
            st.markdown(conteudo)

    # Campo de entrada de texto do usu√°rio
    prompt_usuario = st.chat_input("Como posso te ajudar com C√°lculo?")
    if prompt_usuario:
        # Salva e exibe a mensagem do usu√°rio
        salvar_mensagem(user_id, "user", prompt_usuario)
        with st.chat_message("user"):
            st.markdown(prompt_usuario)

        # Processa a pergunta e obt√©m a resposta da IA
        with st.spinner("DerivaAI est√° pensando..."):
            resposta = chain_memoria.invoke({'memoria': memoria.buffer, 'pergunta': prompt_usuario})
            resposta_texto = resposta.content
        
        # Salva e exibe a resposta da IA
        salvar_mensagem(user_id, "ai", resposta_texto)
        with st.chat_message("ai"):
            st.markdown(resposta_texto)

# --- FUN√á√ÉO PRINCIPAL ---

def main():
    load_css()
    pagina_chat()

if __name__ == "__main__":
    main()